{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyP49yRj2+4ozvMxLS03RzVk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/halfag/LocalLLM/blob/main/Mixtral_Autogen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!apt-get update -q && apt install pciutils -y\n",
        "\n",
        "!curl https://ollama.ai/install.sh | sh\n",
        "!nohup ollama serve &\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lYQ9zmB1Ek7Z"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!ollama pull dolphin-mixtral"
      ],
      "metadata": {
        "id": "KjNK1pygyFrM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install kaleido python-multipart cohere\n",
        "!pip install litellm[proxy_server] pyautogen\n"
      ],
      "metadata": {
        "id": "kpYvpG0zK7qc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!nohup litellm --model ollama/dolphin-mixtral &"
      ],
      "metadata": {
        "id": "PdeB1lwhowN6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install aiohttp pyngrok\n",
        "!ngrok config add-authtoken 2YyeNPLWs82ZVtL13BIV3rZldWj_68suxzPW6XuS7F1fGfeko\n",
        "\n",
        "import os\n",
        "import asyncio\n",
        "\n",
        "# Set LD_LIBRARY_PATH so the system NVIDIA library\n",
        "os.environ.update({'LD_LIBRARY_PATH': '/usr/lib64-nvidia'})\n",
        "\n",
        "async def run_process(cmd):\n",
        "  print('>>> starting', *cmd)\n",
        "  p = await asyncio.subprocess.create_subprocess_exec(\n",
        "      *cmd,\n",
        "      stdout=asyncio.subprocess.PIPE,\n",
        "      stderr=asyncio.subprocess.PIPE,\n",
        "  )\n",
        "\n",
        "  async def pipe(lines):\n",
        "    async for line in lines:\n",
        "      print(line.strip().decode('utf-8'))\n",
        "\n",
        "  await asyncio.gather(\n",
        "      pipe(p.stdout),\n",
        "      pipe(p.stderr),\n",
        "  )\n",
        "\n",
        "await asyncio.gather(\n",
        "    run_process(['ollama', 'serve']),\n",
        "    run_process(['ngrok', 'http', '--log', 'stderr', '11434']),\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXGHOQe5sEMW",
        "outputId": "f3220f1f-c542-400a-ddbe-8420c78405b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.0.3-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (4.0.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp) (3.6)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.0.3\n",
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            ">>> starting ollama serve\n",
            ">>> starting ngrok http --log stderr 11434\n",
            "Error: listen tcp 127.0.0.1:11434: bind: address already in use\n",
            "t=2023-12-21T16:42:40+0000 lvl=info msg=\"no configuration paths supplied\"\n",
            "t=2023-12-21T16:42:40+0000 lvl=info msg=\"using configuration at default config path\" path=/root/.config/ngrok/ngrok.yml\n",
            "t=2023-12-21T16:42:40+0000 lvl=info msg=\"open config file\" path=/root/.config/ngrok/ngrok.yml err=nil\n",
            "t=2023-12-21T16:42:40+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040 allow_hosts=[]\n",
            "t=2023-12-21T16:42:41+0000 lvl=info msg=\"client session established\" obj=tunnels.session obj=csess id=538f0b1481cb\n",
            "t=2023-12-21T16:42:41+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
            "t=2023-12-21T16:42:41+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=command_line addr=http://localhost:11434 url=https://9bb0-34-142-205-14.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pw8QrlRwzSf8"
      },
      "outputs": [],
      "source": [
        "# Code used:\n",
        "import autogen\n",
        "import openai\n",
        "openai.api_type = \"open_ai\"\n",
        "openai.api_key = \"sk-111111111111111111111111111111111111111111111\"\n",
        "openai.api_base = \"http://127.0.0.1:8000/v1\"\n",
        "# openai.api_version = \"2023-05-15\"\n",
        "\n",
        "\n",
        "wizard_config_list = [\n",
        "    {\n",
        "        \"api_key\": \"sk-111111111111111111111111111111111111111111111\",\n",
        "        #\"api_key\": \"NULL\",\n",
        "        # 'api_type': 'open_ai',\n",
        "        'base_url': \"http://127.0.0.1:8000/v1\",\n",
        "        # 'api_version': '2023-05-15',\n",
        "        # \"timeout\": \"600\",\n",
        "        # \"request_timeout\": 8000\n",
        "    },\n",
        "]\n",
        "\n",
        "llama2_config_list = [\n",
        "    {\n",
        "        \"base_url\": \"http://127.0.0.1:8000/v1\",\n",
        "        #\"api_key\": \"NULL\",\n",
        "        \"api_key\": \"sk-111111111111111111111111111111111111111111111\",\n",
        "        # 'api_type': 'open_ai',\n",
        "        # \"timeout\": \"600\",\n",
        "        # \"request_timeout\": 8000\n",
        "    },\n",
        "]\n",
        "\n",
        "\n",
        "coder_llm_config = {\n",
        "    # \"seed\": None,  # change the seed for different trials\n",
        "    \"temperature\": 0,\n",
        "    \"config_list\": llama2_config_list,\n",
        "    # \"timeout\": 600,\n",
        "    # \"request_timeout\": 8000\n",
        "    # \"clear_cache\": True\n",
        "}\n",
        "\n",
        "llama2_llm_config = {\n",
        "    # \"seed\": None,  # change the seed for different trials\n",
        "    # no caching\n",
        "    # \"clear_cache\": True,\n",
        "    \"temperature\": 0,\n",
        "    \"config_list\": llama2_config_list,\n",
        "    # \"timeout\": 600,\n",
        "    # \"request_timeout\": 8000\n",
        "}\n",
        "\n",
        "\n",
        "user_proxy = autogen.UserProxyAgent(\n",
        "   name=\"User_proxy\",\n",
        "   system_message = \"\"\"Reply TERMINATE if the task has been solved at full satisfaction. Otherwise, reply CONTINUE, or the reason why the task is not solved yet.\"\"\",\n",
        "   code_execution_config={\"last_n_messages\": 2, \"work_dir\": \"groupchat\", \"use_docker\":\"True\"},\n",
        "   human_input_mode=\"TERMINATE\"\n",
        ")\n",
        "coder = autogen.AssistantAgent(\n",
        "    name=\"Coder\",\n",
        "    llm_config=llama2_llm_config\n",
        ")\n",
        "pm = autogen.AssistantAgent(\n",
        "    name=\"Product_manager\",\n",
        "    system_message=\"Manage deveolpment of software\",\n",
        "    llm_config=llama2_llm_config\n",
        ")\n",
        "groupchat = autogen.GroupChat(agents=[user_proxy, coder, pm], messages=[], max_round=12)\n",
        "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llama2_llm_config)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_proxy.initiate_chat(manager, message=\"\"\"What's today's date?\"\"\")\n",
        "# type exit to terminate the chat"
      ],
      "metadata": {
        "id": "UhyUA1frUiCn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}